{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo and Sampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from numpy import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange # prints a nice loading bar for the notebook\n",
    "import timeit\n",
    "from numba import jit\n",
    "import copy\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Introduction</span>\n",
    "\n",
    "Monte Carlo (MC) integration and other sampling techniques are important tools for computing complex intgerals that arising in many areas of science. In this lecture, we will study Monte Carlo integration methods and re-visit Markov Chain MC with some specific physics examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Monte Carlo Integration</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical integration uses the rectangle approximation to find the area under a curve.  The analytical notation we are used to seeing for a definite integral\n",
    "\n",
    "$$ \\Large\n",
    "F = \\int_a^b f(x) dx\n",
    "$$\n",
    "\n",
    "can be expressed as a [numerical approximation that adds up $n$ rectangles under the curve $f(x)$](https://mathworld.wolfram.com/NumericalIntegration.html).  The more rectangles used to calculate the area, the better the approximation becomes.\n",
    "\n",
    "Monte Carlo Integration is a process of solving integrals having numerous values to integrate upon. The Monte Carlo process uses the theory of large numbers and random sampling to approximate values that are very close to the actual solution of the integral. \n",
    "\n",
    "Monte Carlo Integration improves above the integration approach by randomly picking which rectangles to add up next and approximating $F$ as $\\langle F^N \\rangle$:\n",
    "\n",
    "$$ \\Large\n",
    "\\langle F^N \\rangle = ~\\frac{1}{N} \\sum_{i=1}^{N} \\frac{f(X_i)}{p(X_i)} \\\\\n",
    "~~~~~~~~~~~~~~~ = ~\\frac{1}{N} \\sum_{i=1}^{N} \\frac{f(X_i)}{1 / (b-a)} \\\\\n",
    "~~~~~~~~~~~~~~~ = ~\\frac{b-a}{N} \\sum_{i=1}^{N} f(X_i)\n",
    "$$\n",
    "$$ \\Large\n",
    "\\Rightarrow\n",
    "~~~\\boxed{\\langle F^N \\rangle = \\frac{b-a}{N} \\sum_{i=1}^{N} f(X_i)} ~~~~~~~~~~~~~ \\text{(Monte Carlo Estimator)}\n",
    "$$\n",
    "\n",
    "where $N$ is the number of times a new value $X_i$ is chosen from a probability distribution for range $a$ to $b$.  Therefore, \n",
    "\n",
    "$$ \\Large\n",
    "{\\text{lim}}_{N‚Üí‚àû} \\langle F^N \\rangle = F\n",
    "$$\n",
    "\n",
    "The question becomes, what is the best way to choose $X_i$ to match the real system?  The goal is to get the best approximation as quickly as possible.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:Violet\">EXAMPLE</span>___: Lets appoximate the definite integral using the Monte Carlo integration method:\n",
    "$$ \\Large\n",
    "\\int \\limits_0^\\pi \\sin x ~dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limits of integration \n",
    "a = 0\n",
    "b = np.pi # gets the value of pi \n",
    "N = 1000\n",
    "\n",
    "# array of zeros of length N \n",
    "ar = np.zeros(N) \n",
    "\n",
    "# iterating over each Value of ar and filling \n",
    "# it with a random value between the limits a \n",
    "# and b \n",
    "for i in range (len(ar)): \n",
    "\tar[i] = random.uniform(a,b) \n",
    "\n",
    "# variable to store sum of the functions of \n",
    "# different values of x \n",
    "integral = 0.0\n",
    "\n",
    "# function to calculate the sin of a particular \n",
    "# value of x \n",
    "def f(x): \n",
    "\treturn np.sin(x) \n",
    "\n",
    "# iterates and sums up values of different functions \n",
    "# of x \n",
    "for i in ar: \n",
    "\tintegral += f(i) \n",
    "\n",
    "# we get the answer by the formula derived adobe \n",
    "ans = (b-a)/float(N)*integral \n",
    "\n",
    "# prints the solution \n",
    "print (\"The value calculated by monte carlo integration is {}.\".format(ans)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value obtained is very close to the actual answer of the integral which is 2.0. \n",
    "\n",
    "Now if we want to visualize the integration using a histogram, we can do so by using the matplotlib library. Again we import the modules, define the limits of integration and write the sin function for calculating the sin value for a particular value of x. Next, we take an array that has variables representing every beam of the histogram. Then we iterate through N values and repeat the same process of creating a zeros array, filling it with random x values, creating an integral variable adding up all the function values, and getting the answer N times, each answer representing a beam of the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limits of integration \n",
    "a = 0\n",
    "b = np.pi # gets the value of pi \n",
    "N = 1000\n",
    "\n",
    "# function to calculate the sin of a particular \n",
    "# value of x \n",
    "def f(x): \n",
    "\treturn np.sin(x) \n",
    "\n",
    "# list to store all the values for plotting \n",
    "plt_vals = [] \n",
    "\n",
    "# we iterate through all the values to generate \n",
    "# multiple results and show whose intensity is \n",
    "# the most. \n",
    "for i in range(N): \n",
    "\t\n",
    "\t#array of zeros of length N \n",
    "\tar = np.zeros(N) \n",
    "\n",
    "\t# iterating over each Value of ar and filling it \n",
    "\t# with a random value between the limits a and b \n",
    "\tfor i in range (len(ar)): \n",
    "\t\tar[i] = random.uniform(a,b) \n",
    "\n",
    "\t# variable to store sum of the functions of different \n",
    "\t# values of x \n",
    "\tintegral = 0.0\n",
    "\n",
    "\t# iterates and sums up values of different functions \n",
    "\t# of x \n",
    "\tfor i in ar: \n",
    "\t\tintegral += f(i) \n",
    "\n",
    "\t# we get the answer by the formula derived adobe \n",
    "\tans = (b-a)/float(N)*integral \n",
    "\n",
    "\t# appends the solution to a list for plotting the graph \n",
    "\tplt_vals.append(ans) \n",
    "\n",
    "# details of the plot to be generated \n",
    "# sets the title of the plot \n",
    "plt.title(\"Distributions of areas calculated\") \n",
    "\n",
    "# 3 parameters (array on which histogram needs \n",
    "plt.hist (plt_vals, bins=30, ec=\"black\") \n",
    "\n",
    "# to be made, bins, separators colour between the \n",
    "# beams) \n",
    "# sets the label of the x-axis of the plot \n",
    "plt.xlabel(\"Areas\") \n",
    "plt.show() # shows the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:Violet\">EXAMPLE</span>___: Lets appoximate the definite integral\n",
    "$$ \\Large\n",
    "\\int \\limits_0^1 x^2 ~dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limits of integration \n",
    "a = 0\n",
    "b = 1\n",
    "N = 1000\n",
    "\n",
    "# array of zeros of length N \n",
    "ar = np.zeros(N) \n",
    "\n",
    "# iterating over each Value of ar and filling \n",
    "# it with a random value between the limits a \n",
    "# and b \n",
    "for i in range(len(ar)): \n",
    "\tar[i] = random.uniform(a, b) \n",
    "\n",
    "# variable to store sum of the functions of \n",
    "# different values of x \n",
    "integral = 0.0\n",
    "\n",
    "# function to calculate the sin of a particular \n",
    "# value of x \n",
    "def f(x): \n",
    "\treturn x**2\n",
    "\n",
    "# iterates and sums up values of different \n",
    "# functions of x \n",
    "for i in ar: \n",
    "\tintegral += f(i) \n",
    "\n",
    "# we get the answer by the formula derived adobe \n",
    "ans = (b-a)/float(N)*integral \n",
    "\n",
    "# prints the solution \n",
    "print(\"The value calculated by monte carlo integration is {}.\".format(ans)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limits of integration \n",
    "a = 0\n",
    "b = 1\n",
    "N = 1000\n",
    "\n",
    "# function to calculate x^2 of a particular value \n",
    "# of x \n",
    "def f(x): \n",
    "\treturn x**2\n",
    "\n",
    "# list to store all the values for plotting \n",
    "plt_vals = [] \n",
    "\n",
    "# we iterate through all the values to generate \n",
    "# multiple results and show whose intensity is \n",
    "# the most. \n",
    "for i in range(N): \n",
    "\t\n",
    "\t# array of zeros of length N \n",
    "\tar = np.zeros(N) \n",
    "\n",
    "\t# iterating over each Value of ar and filling \n",
    "\t# it with a random value between the limits a \n",
    "\t# and b \n",
    "\tfor i in range (len(ar)): \n",
    "\t\tar[i] = random.uniform(a,b) \n",
    "\n",
    "\t# variable to store sum of the functions of \n",
    "\t# different values of x \n",
    "\tintegral = 0.0\n",
    "\n",
    "\t# iterates and sums up values of different functions \n",
    "\t# of x \n",
    "\tfor i in ar: \n",
    "\t\tintegral += f(i) \n",
    "\n",
    "\t# we get the answer by the formula derived adobe \n",
    "\tans = (b-a)/float(N)*integral \n",
    "\n",
    "\t# appends the solution to a list for plotting the \n",
    "\t# graph \n",
    "\tplt_vals.append(ans) \n",
    "\n",
    "# details of the plot to be generated \n",
    "# sets the title of the plot \n",
    "plt.title(\"Distributions of areas calculated\") \n",
    "\n",
    "# 3 parameters (array on which histogram needs \n",
    "# to be made, bins, separators colour between \n",
    "# the beams) \n",
    "plt.hist (plt_vals, bins=30, ec=\"black\") \n",
    "\n",
    "# sets the label of the x-axis of the plot \n",
    "plt.xlabel(\"Areas\") \n",
    "plt.show() # shows the plot \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightBlue\">Multidimensional Monte Carlo integration and variance scaling</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the Monte Carlo variance of the approximation as\n",
    "$$ \\Large\n",
    "v_N = \\frac{1}{N^2} \\sum_{i=1}^N \\Biggl[ (f(x_i) - \\bar{f_N})^2 \\Biggr]\n",
    "$$\n",
    "\n",
    "Also, from the Central Limit Theorem,\n",
    "$$ \\Large\n",
    "\\frac{\\bar{f_N} - E[f(X)]}{\\sqrt{v_N}} \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "The convergence of Monte Carlo integration is $O(\\sqrt{N})$ and independent of the dimensionality. Hence Monte Carlo integration generally beats numerical integration for moderate- and high-dimensional integration since numerical integration (quadrature) converges as $O(N^d)$. Even for low dimensional problems, Monte Carlo integration may have an advantage when the volume to be integrated is concentrated in a very small region and we can use information from the distribution to draw samples more often in the region of importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:Violet\">EXAMPLE</span>___: 3-D Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Large\n",
    "\\int\\limits_{x_0}^{x_1} ~ \\int\\limits_{y_0}^{y_1} ~ \\int\\limits_{z_0}^{z_1} f(x, y, z) dx~dy~dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform 3-D random variable:\n",
    "$$\\Large\n",
    "X_i \\sim p(x,y,z) = \\frac{1}{x_1 - x_0} ~ \\frac{1}{y_1 - y_0} ~ \\frac{1}{z_1 - z_0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic 3-D estimator:\n",
    "$$ \\Large\n",
    "F_N = \\frac{(x_1 - x_0)(y_1 - y_0)(z_1 - z_0)}{N} ~ \\sum\\limits_{i=1}^{N} f(X_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generalizes to abitrary N-dimensional PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightBlue\">Variance and Bias in Monte Carlo integration</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are often interested in knowing how many iterations it takes for Monte Carlo integration to ‚Äúconverge‚Äù and the accuracy of the calculation. To do this, we would like some estimate of the variance and to ensure it is unbiased. It is useful to inspect such plots. One simple way to get confidence intervals for the plot of Monte Carlo estimate against number of iterations is simply to do many such simulations.\n",
    "\n",
    "___<span style=\"color:Violet\">EXAMPLE</span>___: Using Monte Carlo methods, estimate the integral of the function\n",
    "$$ \\Large\n",
    "f(x) = x \\cos 7x + \\sin 13x, \\ \\ \\ 0 \\le x \\le 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x * np.cos(71*x) + np.sin(13*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "plt.plot(x, f(x))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Single Monte Carlo estimate</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = f(np.random.random(n))\n",
    "y = 1.0/n * np.sum(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Using multiple independent sequences to monitor convergence</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vary the sample size from 1 to 100 and calculate the value of $y=\\sum x / n$ for 1000 replicates. We then plot the 2.5th and 97.5th percentile of the 1000 values of $y$ to see how the variation in $y$ changes with sample size. The blue lines indicate the 2.5th and 97.5th percentiles, and the red line a sample path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "reps = 1000\n",
    "\n",
    "x = f(np.random.random((n, reps)))\n",
    "y = 1/np.arange(1, n+1)[:, None] * np.cumsum(x, axis=0)\n",
    "upper, lower = np.percentile(y, [2.5, 97.5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, n+1), y, c='grey', alpha=0.02)\n",
    "plt.plot(np.arange(1, n+1), y[:, 0], c='red', linewidth=1);\n",
    "plt.plot(np.arange(1, n+1), upper, 'b', np.arange(1, n+1), lower, 'b')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Proof that Monte Carlo Estimator is Unbiased</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is straightforward to prove that the expectation value of the Monte Carlo estimator is the desired integral (i.e. it is unbiased). \n",
    "\n",
    "First recall some properties of the expectation value $E$:\n",
    "$$ \\Large\n",
    "E ~ \\Biggl[ \\sum\\limits_i Y_i \\Biggr] = \\sum\\limits_i E\\bigl[ Y_i \\bigr]   ~~~~~~~~~~~~~~~~~~~~~ E~\\Bigl[ aY \\Bigr] = a E~\\Bigl[ Y \\Bigr]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then\n",
    "$$ \\Large E~\\Bigl[ F_N \\Bigr] = E ~ \\Biggl[ \\frac{1}{N} ~ \\sum\\limits_{i=1}^N \\frac{f(X_i)}{p(X_I)} \\Biggr] \\\\\n",
    "~~~~~~~~~~~~~~ = \\frac{1}{N} ~ \\sum\\limits_{i=1}^N ~E~ \\Biggl[ \\frac{f(X_i)}{p(X_I)} \\Biggr] \\\\\n",
    "~~~~~~~~~~~~~~~~~~~~~~~ = \\frac{1}{N} ~ \\sum\\limits_{i=1}^N ~\\int\\limits_a^b \\frac{f(x)}{p(x)} ~p(x) ~dx \\\\\n",
    "~~~~~~~~~~~~~~ = \\frac{1}{N} ~ \\sum\\limits_{i=1}^N ~\\int\\limits_a^b f(x)  ~dx \\\\\n",
    "~~ = \\int\\limits_a^b f(x)  ~dx \\\\\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightBlue\">Change of Variables</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cauchy distribution is given by\n",
    "$$ \\Large\n",
    "f(x) = \\frac{1}{\\pi (1 + x^2)}, \\ \\ -\\infty \\lt x \\lt \\infty\n",
    "$$\n",
    "\n",
    "Suppose we want to integrate the tail probability $P(X>3)$ using Monte Carlo. One way to do this is to draw many samples form a Cauchy distribution, and count how many of them are greater than 3, but this is extremely inefficient.\n",
    "\n",
    "Only 10% of samples will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_true = 1 - stats.cauchy().cdf(3)\n",
    "print(h_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "x = stats.cauchy().rvs(n)\n",
    "h_mc = 1.0/n * np.sum(x > 3)\n",
    "h_mc, np.abs(h_mc - h_true)/h_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">A change of variables lets us use 100% of draws</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to estimate the quantity\n",
    "$$ \\Large\n",
    "\\int_3^\\infty \\frac{1}{\\pi (1 + x^2)} dx\n",
    "$$\n",
    "\n",
    "Using the substitution $y=3/x$ (and a little algebra), we get\n",
    "$$ \\Large\n",
    "\\int_0^1 \\frac{3}{\\pi(9 + y^2)} dy\n",
    "$$\n",
    "\n",
    "Hence, a much more efficient MC estimator is\n",
    "$$ \\Large\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\frac{3}{\\pi(9 + y_i^2)}\n",
    "$$\n",
    "\n",
    "where $y_i \\sim \\square(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = stats.uniform().rvs(n)\n",
    "h_cv = 1.0/n * np.sum(3.0/(np.pi * (9 + y**2)))\n",
    "h_cv, np.abs(h_cv - h_true)/h_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightBlue\">Monte Carlo Swindles</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from change of variables, there are several general techniques for variance reduction, sometimes known as Monte Carlo \"swindles\" since these methods improve the accuracy and convergence rate of Monte Carlo integration without increasing the number of Monte Carlo samples. Some Monte Carlo swindles are:\n",
    "\n",
    "- importance sampling\n",
    "\n",
    "- stratified sampling\n",
    "\n",
    "- control variates\n",
    "\n",
    "- antithetic variates\n",
    "\n",
    "- conditioning swindles including Rao-Blackwellization and independent variance decomposition\n",
    "\n",
    "Most of these techniques are not particularly computational in nature, so we will not cover them in the course. I expect you will learn them elsewhere. We will illustrate importance sampling and antithetic variables here as examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Antithetic variables</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind antithetic variables is to choose two sets of random numbers that are negatively correlated, then take their average, so that the total variance of the estimator is smaller than it would be with two sets of independent and identically distributed (IID) random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x * np.cos(71*x) + np.sin(13*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import sin, cos, symbols, integrate\n",
    "\n",
    "x = symbols('x')\n",
    "sol = integrate(x * cos(71*x) + sin(13*x), (x, 0,1)).evalf(16)\n",
    "print(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just vanilla Monte Carlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "u = np.random.random(n)\n",
    "x = f(u)\n",
    "y = 1.0/n * np.sum(x)\n",
    "y, abs(y-sol)/sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using antithetic variables for the first half of $u$ supplemented with $1-u$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.r_[u[:n//2], 1-u[:n//2]]\n",
    "x = f(u)\n",
    "y = 1.0/n * np.sum(x)\n",
    "y, abs(y-sol)/sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works because the random draws are now negatively correlated, and hence the sum of the variances will be less than in the IID case, while the expectation is unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Importance Sampling</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary Monte Carlo sampling evaluates\n",
    "\n",
    "$$ \\Large\n",
    "E[g(X)] = \\int_X g(x)\\, p(x) \\, dx\n",
    "$$\n",
    "\n",
    "Using another distribution $h(x)$ which is the so-called ‚Äúimportance function‚Äù, we can rewrite the above expression as an expectation with respect to $h$\n",
    "\n",
    "$$ \\Large\n",
    "E_p[f(x)] \\ = \\ \\int_X f(x) \\frac{p(x)}{h(x)} h(x) dx \\ = \\ E_h\\left[ \\frac{f(X) p(X)}{h(X)} \\right] \n",
    "$$\n",
    "\n",
    "giving us the new estimator\n",
    "\n",
    "$$ \\Large\n",
    "\\bar{f_n} = \\frac{1}{N} \\sum_{i=1}^n \\frac{p(x_i)}{h(x_i)} f(x_i)\n",
    "$$\n",
    "\n",
    "where $x_i \\sim f$ is a draw from the density $h$.\n",
    "\n",
    "This is helpful if the distribution $h$ has a similar shape as the function $f(x)$ that we are integrating over, since we will draw more samples from places where the integrand makes a larger or more ‚Äúimportant‚Äù contribution. This is very dependent on a good choice for the importance function $h$.\n",
    "\n",
    "Two simple choices for $h$ are scaling\n",
    "\n",
    "$$ \\Large\n",
    "h(x) = \\frac{1}{a} ~p(x/a)\n",
    "$$\n",
    "\n",
    "and translation\n",
    "\n",
    "$$ \\Large\n",
    "h(x) = p ~(x - a)\n",
    "$$\n",
    "\n",
    "In these cases, the parameter a is typically chosen using some adaptive algorithm, giving rise to adaptive importance sampling. Alternatively, a different distribution can be chosen as shown in the example below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:Violet\">EXAMPLE</span>___: Suppose we want to estimate the tail probability of $\\square (0,1)$ for $P(X>5)$. \n",
    "\n",
    "Regular MC integration using samples from $\\square (0,1)$ is hopeless since nearly all samples will be rejected. However, we can use the exponential density truncated at 5 as the importance function and use importance sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(4, 10, 100)\n",
    "plt.plot(x, stats.expon(5).pdf(x))\n",
    "plt.plot(x, stats.norm().pdf(x))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Expected answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect about 3 draws out of 10,000,000 from $\\square (0,1)$ to have a value greater than 5. Hence simply sampling from $\\square (0,1)$ is hopelessly inefficient for Monte Carlo integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%precision 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_true = 1 - stats.norm().cdf(5)\n",
    "h_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Using direct Monte Carlo integration</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "y = stats.norm().rvs(n)\n",
    "h_mc = 1.0/n * np.sum(y > 5)\n",
    "# estimate and relative error\n",
    "h_mc, np.abs(h_mc - h_true)/h_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Tan\">Using importance sampling</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "y = stats.expon(loc=5).rvs(n)\n",
    "h_is = 1.0/n * np.sum(stats.norm().pdf(y)/stats.expon(loc=5).pdf(y))\n",
    "# estimate and relative error\n",
    "h_is, np.abs(h_is- h_true)/h_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightBlue\">Quasi-random numbers</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the convergence of Monte Carlo integration is $O(\\sqrt{N})$. One issue with simple Monte Carlo is that randomly chosen points tend to be clumped. Clumping reduces accuracy since nearby points provide little additional information about the function begin estimated. One way to address this is to split the space into multiple integration regions, then sum them up. This is known as <span style=\"color:Violet\">stratified sampling</span>. Another alternative is to use <span style=\"color:Violet\">quasi-random numbers</span> which fill space more efficiently than random sequences.\n",
    "\n",
    "It turns out that if we use quasi-random or low discrepancy sequences, we can get convergence approaching $O(1/N)$. There are several such generators, but their use in statistical settings is limited to cases where we are integrating with respect to uniform distributions. The regularity can also give rise to errors when estimating integrals of periodic functions. However, these quasi-Monte Carlo methods are used in computational finance models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ghalton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ghalton\n",
    "\n",
    "gen = ghalton.Halton(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "xs = np.random.random((100,2))\n",
    "plt.scatter(xs[:, 0], xs[:,1])\n",
    "plt.axis([-0.05, 1.05, -0.05, 1.05])\n",
    "plt.title('Pseudo-random', fontsize=20)\n",
    "plt.subplot(122)\n",
    "ys = np.array(gen.get(100))\n",
    "plt.scatter(ys[:, 0], ys[:,1])\n",
    "plt.axis([-0.05, 1.05, -0.05, 1.05])\n",
    "plt.title('Quasi-random', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_true = 1 - stats.cauchy().cdf(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "x = stats.uniform().rvs((n, 5))\n",
    "y = 3.0/(np.pi * (9 + x**2))\n",
    "h_mc = np.sum(y, 0)/n\n",
    "list(zip(h_mc, 100*np.abs(h_mc - h_true)/h_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen1 = ghalton.Halton(1)\n",
    "x = np.reshape(gen1.get(n*5), (n, 5))\n",
    "y = 3.0/(np.pi * (9 + x**2))\n",
    "h_qmc = np.sum(y, 0)/n\n",
    "list(zip(h_qmc, 100*np.abs(h_qmc - h_true)/h_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightBlue\">Vegas Method</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VEGAS algorithm, due to G. Peter Lepage, is a method for reducing error in Monte Carlo simulations by using a known or approximate probability distribution function to concentrate the search in those areas of the integrand that make the greatest contribution to the final integral. \n",
    "\n",
    "The VEGAS algorithm is based on importance sampling. It samples points from the probability distribution described by the function $|f|$ so that the points are concentrated in the regions that make the largest contribution to the integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:Violet\">EXAMPLE</span>___: 4-D Monte Carlo integration with Vegas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we illustrate the use of [vegas](https://vegas.readthedocs.io/en/latest/vegas.html#module-vegas) by estimating the following integral:\n",
    "$$ \\Large\n",
    "C \\int\\limits_{-1}^{1} ~dx_0 ~ \\int\\limits_{0}^{1} ~dx_1 ~ \\int\\limits_{0}^{1} ~dx_2 ~ \\int\\limits_{0}^{1} ~dx_3 ~ e^{-100 \\sum_d (x_d - 0.5)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the integrand $f(x)$ where $x[d]$ specifies a point in the 4-dimensional space. \n",
    "\n",
    "We then create an integrator `integ` which is an integration operator that can be applied to any 4-dimensional function. It is where we specify the integration volume. \n",
    "\n",
    "Finally we apply `integ` to our integrand $f(x)$, telling the integrator to estimate the integral using `nitn=10` iterations of the vegas algorithm, each of which uses no more than `neval=1000` evaluations of the integrand. Each iteration produces an independent estimate of the integral. \n",
    "\n",
    "The final estimate is the weighted average of the results from all 10 iterations, and is returned by `integ(f ...)`. The call `result.summary()` returns a summary of results from each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vegas\n",
    "\n",
    "import vegas\n",
    "\n",
    "def f(x):\n",
    "    dx2 = 0\n",
    "    for d in range(4):\n",
    "        dx2 += (x[d] - 0.5) ** 2\n",
    "    return np.exp(-dx2 * 100.) * 1013.2118364296088\n",
    "\n",
    "# seed the random number generator so results reproducible\n",
    "np.random.seed((1, 2, 3))\n",
    "\n",
    "# assign integration volume to integrator\n",
    "integ = vegas.Integrator([[-1., 1.], [0., 1.], [0., 1.], [0., 1.]])\n",
    "\n",
    "# adapt to the integrand; discard results\n",
    "integ(f, nitn=5, neval=1000)\n",
    "\n",
    "# do the final integral\n",
    "result = integ(f, nitn=10, neval=1000)\n",
    "print(result.summary())\n",
    "print('result = %s    Q = %.2f' % (result, result.Q))\n",
    "integ.map.show_grid(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Markov Chains and MCMC: A Brief Review</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Markov chain Monte Carlo (MCMC)](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) is a powerful class of methods to sample from probability distributions known only up to an (unknown) normalization constant. \n",
    "\n",
    "Recall the utility of such sampling: this is useful when you are either interested in the samples themselves (for example, inferring unknown parameters in Bayesian inference) or you need them to approximate expected values of functions w.r.t. to a probability distribution (for example, calculating thermodynamic quantities from the distribution of microstates in statistical physics). Sometimes, only the mode of a probability distribution is of primary interest. In this case, it's obtained by numerical optimization so full sampling is not necessary.\n",
    "\n",
    "It turns out that sampling from any but the most basic probability distributions is a difficult task. [Inverse transform sampling](https://en.wikipedia.org/wiki/Inverse_transform_sampling) is an elementary method to sample from probability distributions, but requires the cumulative distribution function, which in turn requires knowledge of the, generally unknown, normalization constant. Now in principle, you could just obtain the normalization constant by numerical integration, but this quickly gets infeasible with an increasing number of dimensions. [Rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling) does not require a normalized distribution, but efficiently implementing it requires a good deal of knowledge about the distribution of interest, and it suffers strongly from the curse of dimensionality, meaning that its efficiency decreases rapidly with an increasing number of variables. That's when you need a smart way to obtain representative samples from your distribution which doesn't require knowledge of the normalization constant. MCMC algorithms are a class of methods which do exactly that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a Markov chain is a random sequence of states in some state space in which the probability of picking a certain state next depends only on the current state in the chain and not on the previous history: it is memory-less.\n",
    "\n",
    "Under certain conditions, a Markov chain has a unique stationary distribution of states to which it will converge after a certain number of states. From that number on, states in the Markov chain will be distributed according to the invariant distribution.\n",
    "MCMC algorithms work by constructing a Markov chain with the probability distribution you want to sample from as the stationary distribution.\n",
    "\n",
    "In order to sample from a distribution $f(x)$, a MCMC algorithm constructs and simulates a Markov chain whose stationary distribution is $f(x)$, meaning that, after an initial \"burn-in\" phase, the states of that Markov chain are distributed according to $f(x)$. We thus just have to store the states to obtain samples from $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:Violet\">EXAMPLE</span>___: Sampling weather states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just for fun (and for illustration), let's quickly whip up a Markov chain which has a unique stationary distribution. Let's for now consider both a discrete state space and discrete \"time\". The key quantity characterizing a Markov chain is the transition operator $T(x_{i+1}|x_i)$ which gives you the probability of being in state $x_{i+1}$ at time $i+1$ given that the chain is in state $x_i$ at time $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Markov chain will hop around on a discrete state space which is made up from three weather states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = (\"sunny\", \"cloudy\", \"rainy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a discrete state space, the transition operator is just a matrix.\n",
    "Columns and rows correspond, in our case, to sunny, cloudy, and rainy weather.\n",
    "We pick more or less sensible values for all transition probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = np.array(((0.6, 0.3, 0.1),\n",
    "                              (0.3, 0.4, 0.3),\n",
    "                              (0.2, 0.3, 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows indicate the states the chain might currently be in and the columns the states the chains might transition to.\n",
    "If we take one \"time\" step of the Markov chain as one hour, then, if it's sunny, there's a 60% chance it stays sunny in the next hour, a 30% chance that in the next hour we will have cloudy weather and only a 10% chance of rain immediately after it had been sunny before.\n",
    "This also means that each row has to sum up to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that wee have defined all of these transition probabilities we have made a Markov chain such as the one below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-dap/DataAnalysisForPhysicists/main/img/MonteCarloSampleMethods-MC_weather.png\" width=400 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our Markov chain for a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_steps = 20000\n",
    "states = [0]\n",
    "for i in range(n_steps):\n",
    "    states.append(np.random.choice((0, 1, 2), p=transition_matrix[states[-1]]))\n",
    "states = np.array(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can monitor the convergence of our Markov chain to its stationary distribution by calculating the empirical probability for each of the states as a function of chain length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def despine(ax, spines=('top', 'left', 'right')):\n",
    "    for spine in spines:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "width = 1000\n",
    "offsets = range(1, n_steps, 5)\n",
    "for i, label in enumerate(state_space):\n",
    "    ax.plot(offsets, [np.sum(states[:offset] == i) / offset \n",
    "            for offset in offsets], label=label)\n",
    "ax.set_xlabel(\"number of steps\")\n",
    "ax.set_ylabel(\"empirical probability\")\n",
    "ax.legend(frameon=False)\n",
    "despine(ax, ('top', 'right'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we provide a brief review on how to use Markov Chain Monte Carlo (MCMC) to compute expectation values, and contrast with Monte Carlo integration. In statistics, we study the properties of random variables $x$ and their probability distributions $P(x)$.  We often care about computing expectation values of functions of the random variable such as\n",
    "$$ \\Large\n",
    "\\langle f(x) \\rangle = \\sum_x f(x) P(x)\n",
    "$$\n",
    "\n",
    "In general, the probability distribution $P(x)$ can be very complicated and difficult to sample efficiently. In this case, we can use Markov chains to represent the probability distribution $P(x)$.   It is possible to pick the transition probabilities of the Markov chain so that the chain‚Äôs equilibrium probability distribution is the desired probability distribution $P(x)$. Once we have a correctly defined Markov chain, we can sample it to obtain a set of sampled states $x_1,x_2,...,x_T$.  These sampled states can be used to estimate expectation values in the following way\n",
    "$$ \\Large\n",
    "\\langle f(x) \\rangle \\approx \\frac{1}{T_{max}} \\sum_{t=1}^{T_{max}} f(x_t)\n",
    "$$\n",
    "\n",
    "This should look familiar! However, you should be clear on the distinction between Monte Carlo and Markov chains:\n",
    "- Monte Carlo methods are ways to evaluate integrals using random numbers. \n",
    "- Markov chains are used to sample complicated probability distributions.\n",
    "\n",
    "When Monte Carlo is used to integrate a probability distribution specified by a Markov chain, then it is called <span style=\"color:Violet\">Markov Chain Monte Carlo</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Metropolis Algorithm</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Chain Monte Carlo methods date back to a [seminal paper by Metropolis et al.](https://pdfs.semanticscholar.org/7b3d/c9438227f747e770a6fb6d7d7c01d98725d6.pdf), who developed the first MCMC algorithm, correspondingly called [Metropolis algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm). It was proposed in 1953 by Edward Teller, Nicholas Metropolis, and others at Los Alamos National Laboratory in New Mexico during the early days of scientific computing. Teller and his physics colleagues were interested in using MCMC to calculate the thermodynamic properties of a weakly interacting classical gas, which was very difficult to calculate analytically. Impressively, more than half a century after its introduction, Metropolis MCMC is still in wide use today in many areas of science, engineering, and statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <span style=\"color:Violet\">Metropolis algorithm</span> samples a Markov chain by proposing moves between states, which are then either accepted or rejected according to a specific criterion. These proposed moves are chosen so that the Markov chain‚Äôs transition probabilities give the correct equilibrium distribution4. In general, Metropolis Markov chain sampling of a probability distribution $P(S)$ works as follows:\n",
    "\n",
    "1. Start at a state $ùëÜ_t$.\n",
    "\n",
    "2. Propose a move to a new state $S'$ based on the current state $S_t$.\n",
    "\n",
    "3. Choose a uniform random number $r$ between 0 and 1.\n",
    "\n",
    "4. If $r<P(S')/P(S_t)$ then accept the proposed move and transition to state $S'$ so that $S_{t+1}=S'$. Otherwise, reject the move and stay at state $S_t$ so that $S_{t+1}=S_t$.\n",
    "\n",
    "5. Increment $t$ and repeat.\n",
    "\n",
    "This Markov chain‚Äôs samples $S_1,...S_{T_{max}}$ are then used to estimate expectation values $\\langle f(S) \\rangle$ in the way  discussed in the previous section. We will use this general Metropolis MCMC framework to sample the Boltzmann distribution, an important probability distribution in thermal physics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">2D Ising Model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Ising Hamiltonian for a Ferromagnetic 2D System</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: For a very good reference on this derivation (and where much of this notation comes from), [see here](https://farside.ph.utexas.edu/teaching/329/lectures/node110.html).  Other good references can be found in the \"Additional Readings\" section.*\n",
    "\n",
    "Consider a system of ferromagnetic atoms, where each atom initially has spin up (+1) or down (-1).  Ferromagnetic means that the system wants to align all of the spins in the same direction to minimize energy, so that all of the spins should be up or all of the spins should be down.  If the net-magnetization of the system is zero, all of the spins cancel each other out.  If the net-magnetization of the system is greater than zero, then some percentage of the magnetic spins are aligned.\n",
    "\n",
    "We want to be able to study the phase transition of the system from disordered (spins randomly oriented) to ordered (spins aligned), and vice versa. In order to do this, let's arrange the spins onto the grid points of a lattice as shown in the figure below.  Note that the picture shown is for a 2D system, but any useful dimensionality can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to show an example lattice with non-interacting spins\n",
    "spin_up_or_down = np.random.choice([0,1], size=(10,10))\n",
    "xv, yv = np.meshgrid(np.linspace(1, 10, 10), np.linspace(1, 10, 10))\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(xv[np.reshape(spin_up_or_down, (10, 10)) == 1],\n",
    "            yv[np.reshape(spin_up_or_down, (10, 10)) == 1],\n",
    "            c='r', label=\"Spin Up\")\n",
    "ax.scatter(xv[np.reshape(spin_up_or_down, (10, 10)) == 0],\n",
    "            yv[np.reshape(spin_up_or_down, (10, 10)) == 0],\n",
    "            c='b', label=\"Spin Down\")\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.axis('equal')\n",
    "plt.title(\"Figure 1. Lattice with non-interacting atoms on grid points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the energy of the $i^{th}$ atom in the system above, we start with the energy of the atom in a magnetic field\n",
    "\n",
    "$$ \\Large\n",
    "\\epsilon_i = \\mu H \\sigma_i\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the atomic magnetic moment, $H$ is an applied magnetic field, and the $\\sigma$ operator \"measures\" the spin of the atom and returns value $+1$ for spin up or $-1$ for spin down.\n",
    "\n",
    "Next, lets add a nearest-neighbors interaction between the particles, with strength $J$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to show add nearest-neighbors interactions to the above figure\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "for x, y in zip(xv, yv):\n",
    "    ax.plot(x, y, 'k', alpha=0.25)\n",
    "    ax.plot(y, x, 'k', alpha=0.25)\n",
    "ax.scatter(xv[np.reshape(spin_up_or_down, (10, 10)) == 1],\n",
    "            yv[np.reshape(spin_up_or_down, (10, 10)) == 1],\n",
    "            c='r', label=\"Spin Up\")\n",
    "ax.scatter(xv[np.reshape(spin_up_or_down, (10, 10)) == 0],\n",
    "            yv[np.reshape(spin_up_or_down, (10, 10)) == 0],\n",
    "            c='b', label=\"Spin Down\")\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.axis('equal')\n",
    "plt.title(\"Figure 2. Adding Nearest Neighbor Interactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To include the energy from each site interacting with the $i$th atom, we can update the above expression:\n",
    "\n",
    "$$ \\Large\n",
    "\\epsilon_i = -\\frac{1}{2}~J\\sum_{j\\in[1,4]} \\sigma_i \\sigma_j + \\mu H \\sigma_i\n",
    "$$\n",
    "\n",
    "where\n",
    "- the $\\frac{1}{2}$ factor ensures that we don't double count the contributions from neighboring atoms.\n",
    "\n",
    "- $J$ is the coupling between nearest neighbors (the gray lines in Figure 2)\n",
    "\n",
    "- The negative sign \"$-$\" in front of the $J$ means that this is a ferromagnetic system\n",
    "\n",
    "- $j$ means that only four nearest neighbors for each atom in the above Figure will be included in the calculation\n",
    "\n",
    "\n",
    "It's important to note here, that the $\\sigma_i$ term can be pulled out of the sum and we can divide by the magnetic moment $\\mu$:\n",
    "\n",
    "$$ \\Large\n",
    "\\mu = -\\frac{1}{2 \\mu}J \\sigma_i \\sum_{j\\in[1,4]} \\sigma_j\n",
    "$$\n",
    "\n",
    "So that the total energy for the $i^\\text{th}$ spin can be written as\n",
    "\n",
    "$$ \\Large\n",
    "\\epsilon_i = \\mu \\sigma H_{eff}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\Large\n",
    "H_{eff} = -\\frac{1}{2 \\mu}J \\sum_{j\\in[1,4]} \\sigma_j + H\n",
    "$$\n",
    "\n",
    "which means that for atom $i$, the only difference in energy for the atom in a spin up state or spin down state is a $+$ or $-$ sign:\n",
    "\n",
    "$$ \\Large\n",
    "\\Large \\epsilon_i\\{\\sigma_i, \\uparrow \\} = - \\epsilon_i\\{\\sigma_i, \\downarrow \\}\n",
    "$$\n",
    "\n",
    "To get the total energy for the system, we sum over all the measured of the observed energies:\n",
    "\n",
    "$$ \\Large\n",
    "E = \\sum_i \\epsilon_i\n",
    "$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this Hamiltonian, we want to understand the expected spin value given the temperature of the system. The probability $p$ that a spin has a particular value can be calculated using the [Maxwell-Boltzmann distribution](https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Supplemental_Modules_(Physical_and_Theoretical_Chemistry)/Kinetics/03%3A_Rate_Laws/3.01%3A_Gas_Phase_Kinetics/3.1.02%3A_Maxwell-Boltzmann_Distributions)\n",
    "\n",
    "$$ \\Large\n",
    "p_{i,\\pm} = \\frac{e^{\\mp \\epsilon_i/k_BT}}{\\sum_i e^{\\epsilon_i /k_B T}}\n",
    "$$\n",
    "\n",
    "where $\\epsilon_i$ is the energy of the atom, and $k_BT$ is the energy environment (also referred to as a temperature bath) of the atom.  The denominator of the fraction normalizes the probability distribution, so that there is a max probability of 1.\n",
    "\n",
    "The average value of the magnetic moment of the atom for a given temperature is\n",
    "\n",
    "$$ \\Large\n",
    "\\langle \\mu \\rangle = \\frac{\\mu p_+ + (-\\mu)p_-}{p_+ + p_-}\n",
    "$$\n",
    "\n",
    "After plugging in the expression for the probability and evaluating the $\\sigma_i$ operators appropriately, we get\n",
    "\n",
    "$$ \\Large\n",
    "~~~~~~~~~~\\langle \\mu \\rangle = \\mu \\frac{e^{- \\mu H/k_BT} - e^{+ \\mu H /k_BT}}{e^{- \\mu H/k_BT} + e^{+ \\mu H/k_BT}} \\\\\n",
    "= \\mu \\tanh(\\frac{\\mu H}{k_B T})\n",
    "$$\n",
    "\n",
    "This is not an expression we can solve **analytically**, <span style=\"color:Violet\">but it can be solved numerically</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Metropolis-Hastings Algorithm Walk-through</span>\n",
    "\n",
    "This algorithm has multiple variations in addition to \"[Metropolis-Hastings](https://pubs.aip.org/aip/jcp/article-abstract/21/6/1087/202680/Equation-of-State-Calculations-by-Fast-Computing?redirectedFrom=fulltext)\", such as \"[Stochastic Simulated Annealing](https://machinelearningmastery.com/simulated-annealing-from-scratch-in-python/)\" and \"[Sequential Simulated Annealing](https://link.springer.com/article/10.1007/s10898-011-9838-3)\".  The \"simulated annealing\" comes from the idea that the system begins in a random state that is equivalent to the system being at infinitely high temperature. The system is then \"submerged\" into a bath of temperature $T$ where $T$ is a finite temperature that \"cools\" the system off.\n",
    "\n",
    "---\n",
    "\n",
    "0. Initialize the system with the following variables:\n",
    "    - $T$: temperature of the \"bath\" which will anneal the system\n",
    "    - $N$: the size of the system\n",
    "    - $s$: the state of each particle in the system\n",
    "    - $J$: the coupling between particles in the system\n",
    "    - $H$: the applied magnetic field\n",
    "1. Select an atom in the system\n",
    "2. Calculate the energy $e_i$ of that atom\n",
    "3. Determine if changing the spin of the atom would decrease the energy\n",
    "4. If it decreases the energy **OR** if the probability of the state in the Boltzmann distribution is greater than a sample from a uniform probability distribution, then change the spin state of the atom.\n",
    "5. Continue with steps 1-4 until the stopping criteria is met.\n",
    "6. Return to step 0 and re-initialize the system with different variables as desired.\n",
    "---\n",
    "Let's go through these steps in code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ___Step 0___: <span style=\"color:Tan\">Initialize the lattice and the simulation</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a square, 10x10 lattice (N = 100). By calling the `np.random.uniform()` function, I know I will get an uniform distribution of values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lattice = np.random.uniform(size=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to prepare the system into spin up and spin down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lattice[lattice>0.5]=1 # any values greater than 0.5, I set equal to 1\n",
    "lattice[lattice !=1]=-1 # anything I don't change before, I make equal to -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing for the lattice is to visually check and see if everything turned out okay.  Every value in the lattice should be $+1$ or $-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(lattice)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to create a temperature variable. In the equations above, the inverse temperature is represented by $1/k_B T$, where $k_B$ is the Boltzmann constant. If $T = \\inf$, then the inverse temperature will zero, and if $T=0$ then the inverse temperature will be $‚àû$.  Accordingly, we can define:\n",
    "\n",
    "$$ \\Large\n",
    "\\beta = 1/k_B T\n",
    "$$\n",
    "\n",
    "where we can artificially restrict $\\beta \\ge 0$ to make calculations simple.  We already know what the system is like at infinite temperature because we are starting out in a thermalized state, so this way as $T ‚Üí 0$, $\\beta$ will become very large.  For now, let's start with $\\beta=0$, just to make sure things are working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0 # thermodynamic inverse temperature variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For right now, let's set $H=0$, $J=1$, and $\\mu=1$.  This means I can omit them from my code, and proceed to step 1 of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ___Step 1___: <span style=\"color:Tan\">Select an atom in the system</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we need to make another coding decision. If we start with the atom at location (0, 0), then there are no neighbors located at (-1, 0) and (0, -1) because of how ```numpy``` matrices are indexed. This same thing will happen for all of the other atoms at the edges of my lattice: they are missing neighbors. So for right now, we will start in the *second* row and *second* column, then stop at the *second-to-last* row and *second-to-last* column. We won't be able to update the edge atoms with the lower energy configurations, but my code will run for right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ___Step 2 - 5___: <span style=\"color:Tan\">See the code comments</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Step 1: double for-loop\n",
    "for r in range(1,9): # the range is from (1,9) instead of (0, 10)\n",
    "  for c in range(1,9): # the range is from (1,9) instead of (0, 10)\n",
    "\n",
    "        # --> Step 2: Calculate the energy of the atom\n",
    "        # the sum over the nearest neighbors is completed first to make it easy to check\n",
    "        sum_NN = (lattice[r-1,c]+lattice[r+1, c]+lattice[r,c+1]+lattice[r,c-1])\n",
    "\n",
    "        # then the total energy for the atom is calculated\n",
    "        E_a = -0.5*lattice[r,c]*sum_NN\n",
    "\n",
    "        # --> Step 3: Change the spin and re-calculate the energy\n",
    "        # Remember we said above that changing the spin just changes the sign\n",
    "        # of the energy\n",
    "        E_b = -1*E_a\n",
    "\n",
    "        # --> Step 4: If the energy decreased or the probability sample\n",
    "        # meets the requirement\n",
    "        if E_b < E_a or np.exp(-(E_b - E_a)*(beta)) > np.random.rand():\n",
    "            # Update the actual spin value in the lattice\n",
    "            lattice[r, c] *= -1\n",
    "\n",
    "        # --> Step 5: The stop condition for this code block is that all of the\n",
    "        # interior atoms have been updated at least once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to visualize the system again to see if anything changed. Since we set the thermodynamic temperature at $\\beta=0 ‚Üí T = \\infty$, it should still be in a mixed state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(lattice)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we implemented a Monte Carlo Markov Chain (MCMC) to update the atom's spin state. The state transition was considered based on sampling from both the Boltzmann distribution and the uniform distribution. The MCMC simulation allowed us to implement the Hamiltonian model for how we think the sytem should behave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:Violet\">EXAMPLE</span>___: Temperature Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good!\n",
    "\n",
    "Let's make some improvements to the code, and simulate the model for a range of temperatures.\n",
    "\n",
    "In the code below, the variable `sqrt_N` = $\\sqrt{N}$.  Since we have $N$ atoms in the system, the lattice has dimension $\\sqrt{N} \\times \\sqrt{N}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the lattice a little larger\n",
    "sqrt_N = 25\n",
    "\n",
    "# Create a new lattice\n",
    "init_lattice = np.random.uniform(size=(sqrt_N,sqrt_N))\n",
    "\n",
    "#mask lattice\n",
    "init_lattice[init_lattice>0.5]=1\n",
    "init_lattice[init_lattice !=1]=-1\n",
    "\n",
    "# A new step here to create non-interacting atoms around the edge by padding\n",
    "# the array edges with zeroes.  This way, we can iterate over all N atoms in the\n",
    "# system without causing an out-of-bounds error\n",
    "lattice = np.zeros((sqrt_N+2, sqrt_N+2))\n",
    "lattice[1:sqrt_N+1, 1:sqrt_N+1] = init_lattice\n",
    "\n",
    "# Define a range of temperatures to test\n",
    "beta = np.linspace(0, 2, 1000)\n",
    "\n",
    "# Empty variable to hold the magnetism calculations\n",
    "M = []\n",
    "\n",
    "# For each temperature\n",
    "for temp in tqdm(beta):\n",
    "\n",
    "    # Repeat the MCMC step 100 times to make sure the system is stable\n",
    "    for n in range(100):\n",
    "\n",
    "        [rows, cols] = lattice.shape # Figure out the size of the lattice\n",
    "\n",
    "        for r in range(1,rows-1): # keep the neighbors inside the region\n",
    "            for c in range(1,cols-1):\n",
    "\n",
    "                # sum over the nearest neighbors\n",
    "                sum_NN = (lattice[r-1,c]+lattice[r+1, c]+lattice[r,c+1]+lattice[r,c-1])\n",
    "\n",
    "                # calculate the energy\n",
    "                E_a = -0.5*lattice[r,c]*sum_NN\n",
    "\n",
    "                # re-calculate the energy for a spin state change\n",
    "                E_b = -1*E_a\n",
    "\n",
    "                # choose whether to keep the new state or not\n",
    "                if E_b < E_a or (np.exp(-(E_b - E_a)*temp) > np.random.rand()):\n",
    "                    lattice[r, c] *= -1\n",
    "\n",
    "    # After the system is stable, calculate the net magnetism by summing over\n",
    "    # all of the spin values and averaging them\n",
    "    M.append(np.abs(np.sum(np.sum(lattice)))/(sqrt_N*sqrt_N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the final lattice to see how the overall spin state has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(lattice)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the net-magnetism as a function of temperature, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(beta, M, s=1)\n",
    "ax.set_xlabel(r\"Inverse Temperature $\\beta$\", )\n",
    "ax.set_ylabel(\"Net Magnetism M\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows a noisy transition from 0 net magnetism at $\\beta=0$ (or $T=\\infty$), to completely magnetized by $\\beta=2$.  The phase transition resembles the [$tanh$](https://mathworld.wolfram.com/HyperbolicTangent.html) function, but centered around 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Acknowledgements\n",
    "* Initial version: Mark Neubauer\n",
    "* From APS DSECOP materials and https://people.duke.edu/~ccc14/sta-663-2016/15C_MonteCarloIntegration.html\n",
    "\n",
    "¬© Copyright 2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
