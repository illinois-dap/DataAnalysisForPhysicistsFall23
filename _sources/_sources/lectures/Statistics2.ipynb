{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### <span style=\"color:LightGreen\">Correlation vs. Independence</span>"
      ],
      "metadata": {
        "id": "1n-I_SpV1JPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last time we discussed the correlation and covariance between two variables:\n",
        "The **correlation between x and y** is defined as:\n",
        "$$ \\Large\n",
        "\\text{Corr}_{xy} \\equiv \\langle\\left( x - \\overline{x}\\right) \\left( y - \\overline{y}\\right)\\rangle \\; .\n",
        "$$\n",
        "\n",
        "A useful combination of the correlation and variances is the **correlation coefficient**,\n",
        "\n",
        "$$ \\Large\n",
        "\\rho_{xy} \\equiv \\frac{\\text{Corr}_{x,y}}{\\sigma_x \\sigma_y} \\; ,\n",
        "$$\n",
        "\n",
        "It is tempting to think that if two variables are uncorrelated, they are independent.  However that is not the case.  \n",
        "\n",
        "Let's take the simple case two random variables X and Y where\n",
        "$X = x$ and $Y = x^2$.  Clearly these two variables are not independent.  If you know any value of x, you can determine y and vise versa.  Let's use the machinary from last week to calculate the correlation coefficient with x sampled from a Guassian centered at $x=0$"
      ],
      "metadata": {
        "id": "SixDRJhO30yJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhdN4Hmivkgg"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "from scipy.stats import moment\n",
        "from scipy.stats import poisson\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import subprocess\n",
        "from random import gauss\n",
        "\n",
        "\n",
        "ntries = 10000\n",
        "gaussarray = np.array([])\n",
        "gaussarray_sq = np.array([])\n",
        "\n",
        "for _ in range(ntries):\n",
        "  value = gauss(0, 2)\n",
        "  if value > -100:\n",
        "    gaussarray = np.append(gaussarray,value)\n",
        "    gaussarray_sq = np.append(gaussarray_sq,value*value)\n",
        "\n",
        "\n",
        "print(np.corrcoef(gaussarray, gaussarray_sq))\n",
        "plt.scatter(gaussarray, gaussarray_sq, s=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot looks sensible, but the correlation coefficent is approximately zero.  Running more events isn't going to give us the strong correlation we might expect.  We can clearly see that given a value of X, Y is uniquely determined.\n",
        "\n",
        "Let's take a look at this mathematically for our case of $X = x$ and $Y = x^2$ with x being symmetric around the origin.  In that case:\n",
        "$$\n",
        "\\bar{X} = 0 \\\\\n",
        "\\bar{Y} = \\langle x^2 \\rangle\n",
        "$$\n",
        "plugging in what we know from last time:\n",
        "$$\n",
        "\\bar{Y} = \\sigma^2_x\n",
        "$$\n",
        "so the correlation between X and Y is:\n",
        "$$\\text{Corr}_{xy} = \\langle\\left( x\\right) \\left( x^2 - \\overline{x^2}\\right)\\rangle \\\\\n",
        "= \\langle (x\\left( x^2 - \\sigma^2\\right)\\rangle \\\\\n",
        "=\\langle x^3 - x^2\\langle x\\rangle \\rangle = 0.\n",
        "$$\n",
        "\n",
        "This works because the function is symmetric about $x=0$.  Let's look at the same Gaussian centered at the origin, but only with the requirement that $x>0$.\n",
        "\n"
      ],
      "metadata": {
        "id": "xH-HfuV05TTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ntries = 10000\n",
        "gaussarray = np.array([])\n",
        "gaussarray_sq = np.array([])\n",
        "\n",
        "for _ in range(ntries):\n",
        "  value = gauss(0, 2)\n",
        "  if value > -0:\n",
        "    gaussarray = np.append(gaussarray,value)\n",
        "    gaussarray_sq = np.append(gaussarray_sq,value*value)\n",
        "\n",
        "\n",
        "print(np.corrcoef(gaussarray, gaussarray_sq))\n",
        "plt.scatter(gaussarray, gaussarray_sq, s=10)\n"
      ],
      "metadata": {
        "id": "L7yvcXSYK2lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have seen that if two variables are uncorrelated they are not necessarily independent.  However, if two variables are *independent* then they are uncorrelated.  This means you can't necessarily directly take the correlation between two variables as telling you how much extra information is being provided by the second quantity.  \n",
        "\n",
        "If we just had the datasets themselves and didn't know the $Y = x^2$ relationship set out above, we might calculate the correlation coefficient and think that the two variables are independent.  The plots however make clear that if you know $x$ you directly know $Y$.\n",
        "\n",
        "\n",
        "$\\rho_{xy}$ is known as the Pearson correlation coeffient and is sensitive to the degree of linear correlation.  There are other measures of the correlation (e.g. Spearman's rho).  The Pearson correlation coeffiecent is very common.  Here is a paper from ATLAS where the observable is $\\rho$ between the magnitude of the correlations between particles and the average momentum of the particles produced in the collision of two nuclei: https://arxiv.org/pdf/2205.00039.pdf."
      ],
      "metadata": {
        "id": "sDrltKNkL4tw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <span style=\"color:LightGreen\">Uncertainties on the Mean</span>"
      ],
      "metadata": {
        "id": "8RveOsoXQnVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a dataset of a finite size, there are uncertainties on any of the associated moments.  We'll discuss the special case of counting experiments using the Poisson distribution.  \n",
        "\n",
        "Last week we discussed:\n",
        "\n",
        "$$\\Large P(k; \\lambda) = \\frac{e^{-\\lambda} \\lambda^k}{k!}$$\n",
        "\n",
        "The Poisson distribution is a useful model for describing the statistics of event-counting rates in (uncorrelated) counting measurements (which are ubiquitous in astronomy and particle physics).\n",
        "\n",
        "It's useful to note that the Poisson distribution is defined for integer values of $k$, but $\\lambda$ of course doesn't need to be an integer.\n",
        "\n",
        "$\\lambda$ is the probability times the number of trials ($\\lambda = np$) so it\n",
        "is the expectation value for the mean,\n",
        " but what about the variance (recall $\\sigma_x^2 \\equiv \\langle\\left( x - \\overline{x} \\right)^2\\rangle$ and the standard deviation is $\\sigma_x$)?  It requires a bit of  work to show, but for a Poisson distribution\n",
        " $\\sigma^2 = \\lambda$\n",
        " so the standard deviation is $\\sqrt{\\lambda}$.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NVJimMYoTt-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mu = 4\n",
        "rv = poisson(mu)\n",
        "R = poisson.rvs(mu, size=1000)\n",
        "plt.hist(R,bins=25)\n",
        "print(\"mean: \", np.mean(R), \" & variance: \", np.var(R))"
      ],
      "metadata": {
        "id": "PYAHvyKnUNxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that the relative uncertainty on the mean of a counting experiment is $1/\\sqrt{\\lambda}$. That means\n",
        "that for a counting experiment the uncertainty on the mean\n",
        "becomes smaller with $1/\\sqrt{n}$.  This is really fundamental to much of experimental physics.  \n",
        "If you want 10% precision you need:\n",
        "$$1/\\sqrt{N} = 0.1\\\\\n",
        "N=100$$\n",
        "if you want 1% precision you need:\n",
        "$$1/\\sqrt{N} = 0.01\\\\\n",
        "N=10000$$\n",
        "\n",
        "Precision improves quickly at first as you add data, but then much more slowly.\n"
      ],
      "metadata": {
        "id": "VcY9tyop4EVM"
      }
    }
  ]
}