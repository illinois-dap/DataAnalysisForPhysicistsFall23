{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSl_Py-HA7Kp"
   },
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9LM_dqVA7Kq"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "from random import gauss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1XHv4i_A7Ks"
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from scipy.stats import moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZleQWXDyA7Kt"
   },
   "source": [
    "Data helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcUcL5wyA7Kt"
   },
   "outputs": [],
   "source": [
    "def wget_data(url):\n",
    "    local_path='./tmp_data'\n",
    "    subprocess.run([\"wget\", \"-nc\", \"-P\", local_path, url])\n",
    "\n",
    "def locate_data(name, check_exists=True):\n",
    "    local_path='./tmp_data'\n",
    "    path = os.path.join(local_path, name)\n",
    "    if check_exists and not os.path.exists(path):\n",
    "        raise RuxntimeError('No such data file: {}'.format(path))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTHsISw4A7Kt"
   },
   "source": [
    "## <span style=\"color:Orange\">Get Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmI4XfRDA7Ku"
   },
   "outputs": [],
   "source": [
    "wget_data('https://courses.physics.illinois.edu/phys398dap/fa2023/data/blobs_data.hf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YF5JrLKKA7Ku"
   },
   "source": [
    "## <span style=\"color:Orange\">Load Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "zEZlnMysA7Kv",
    "outputId": "ae8ea498-d873-4fba-b4b0-877f392720db"
   },
   "outputs": [],
   "source": [
    "blobs = pd.read_hdf(locate_data('blobs_data.hf5'))\n",
    "blobs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o5vcWOsA7Kv"
   },
   "source": [
    "## <span style=\"color:Orange\">Statistical Methods</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWgrPN76A7Kv"
   },
   "source": [
    "The boundary between probability and statistics is somewhat gray (similar to astronomy and astrophysics).  However, you can roughly think of statistics as the applied cousin of probability theory.\n",
    "\n",
    "Today we're going to discuss the basics of characterizing a dataset in one or more dimensions using moments.  These include things that you are probably reasonably familiar with, like the standard deviation and the mean and others that are probably farther from your experience.\n",
    "\n",
    "In many areas of science these are important.  \n",
    "\n",
    "-Sometimes you just don't have a model for what a distribution should be and so you just start characterizing it using moments (often starting with the mean and standard deviation).  \n",
    "\n",
    "-Other times you have a model, but the data isn't perfect...maybe you've got a lot of background or other noise\n",
    "\n",
    "-in the case of multiple variables, perhaps you want to understand how two (or more) quantities are correlated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HgrwConA7Kw"
   },
   "source": [
    "### <span style=\"color:LightGreen\">Expectation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2r4-FzQXA7Kw"
   },
   "source": [
    "Given a probability density $P(x,y)$, and an arbitrary function $g(x,y)$ of the same random variables, the **expectation value** of $g$ is defined as:\n",
    "\n",
    "$$ \\Large\n",
    "\\langle g\\rangle \\equiv \\iint dx dy\\, g(x,y) P(x,y) \\; .\n",
    "$$\n",
    "\n",
    "Note that the result is just a number, and not a function of either $x$ or $y$. Also, $g$ might have dimensions, which do not need to match those of the probability density $P$, so $\\langle g\\rangle$ generally has dimensions.\n",
    "\n",
    "Sometimes the assumed PDF is indicated with a subscript, $\\langle g\\rangle_P$, which is helpful, but more often not shown.\n",
    "\n",
    "When $g(x,y) = x^n$, the resulting expectation value is called a **raw moment of x**. (The same definitions apply to $y$, via $g(x,y) = y^n$.) The case $n=0$ is just the normalization integral $\\langle \\mathbb{1}\\rangle = 1$.  Low-order moments have familiar names:\n",
    " - $n=1$ yields the **mean of x**, $\\overline{x} \\equiv \\langle x\\rangle$.\n",
    " - $n=2$ yields the **root-mean square (RMS) of x**, $\\langle x^2\\rangle$.\n",
    "\n",
    " The raw moments can be defined in a similar way for any positive integer power of x.\n",
    "\n",
    "Additionally, it is useful to define moments centered about the mean\n",
    " of the distribution.  These **central moments** of order $n$ are defined as $\\langle (x - \\overline{x})^n \\rangle$.  The $n=1$ central moment is always 0. The\n",
    " $n=2$ central moment is the **variance of x**, which combines the mean and RMS,\n",
    "\n",
    "$$ \\Large\n",
    "\\sigma_x^2 \\equiv \\langle\\left( x - \\overline{x} \\right)^2\\rangle \\; ,\n",
    "$$\n",
    "\n",
    "where $\\sigma_x$ is called the **standard deviation of x**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-ifViUzA7Kw",
    "outputId": "31064c9d-269a-4554-88db-a708d3cb3ac8"
   },
   "outputs": [],
   "source": [
    "np.mean(blobs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Wf6gOmAA7Kw",
    "outputId": "bd44c815-51e7-419d-9fde-d724d0219fdc"
   },
   "outputs": [],
   "source": [
    "np.var(blobs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Es77bep7A7Kw",
    "outputId": "e9619dee-f9e9-421f-c780-8e9b54a7586b"
   },
   "outputs": [],
   "source": [
    "np.std(blobs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXwnWUUjA7Kw"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(np.std(blobs, axis=0) ** 2, np.var(blobs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oWJKlWGA7Kw"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    np.mean((blobs - np.mean(blobs, axis=0)) ** 2, axis=0),\n",
    "    np.var(blobs, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNsu69-UDezV"
   },
   "source": [
    "**Characterization of Measurements**\n",
    "\n",
    "The mean and standard deviations are among the most used quantities in physics.  If you know these quantities about a single dimensional dataset, then you actually know quite a lot, regardless of the details of the distribution itself.  \n",
    "\n",
    "The standard deviation divided by the mean is a usual way of quantifying the precision of a measurement.  For example, in high-energy physics, a calorimeter is a device for measuring\n",
    "particle energy.  \n",
    "-Here is a paper about a calorimeter built at Illinois showing the mean energy measured as a function of the particle energy and showing the\n",
    "standard deviation of the measured energy distribution divided by the mean: https://arxiv.org/pdf/2003.13685.pdf (Fig. 8).  \n",
    "-To go back to the QCD jets from a couple weeks ago, here is a paper showing the mean energy response of the ATLAS calorimeters to jets (the \"JES\") and the standard deviation divided by the mean (the \"JER\") https://arxiv.org/pdf/2205.00682.pdf (Fig. 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "zHzNL_kE3CZb",
    "outputId": "4e0cd8f2-ed5c-48a7-bf9c-366fc8a3003f"
   },
   "outputs": [],
   "source": [
    "ntries = 1000\n",
    "gaussarray_wide = np.array([])\n",
    "gaussarray_narrow = np.array([])\n",
    "\n",
    "for _ in range(ntries):\n",
    "\tvalue = gauss(1, 0.2)\n",
    "\tgaussarray_narrow = np.append(gaussarray_narrow,value)\n",
    "\n",
    "for _ in range(ntries):\n",
    "\tvalue = gauss(1, 2)\n",
    "\tgaussarray_wide = np.append(gaussarray_wide,value)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('wide and narrow gaussians')\n",
    "ax1.hist(gaussarray_wide, bins=40, range=[-5,5])\n",
    "ax2.hist(gaussarray_narrow, bins=40, range=[-5,5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMWeC_qxA7Kx"
   },
   "source": [
    "Finally, standardized moments can be defined.  These are central moments normalized by the standard deviation:\n",
    "$$ \\Large\n",
    " \\tilde{x}_N \\equiv \\langle\\left( x - \\overline{x} \\right)^n\\rangle / \\sigma_{x}^n \\; ,\n",
    "$$\n",
    "\n",
    " This makes them scale-invarient and provide some information about the shape of the distribution.  The first standard moment, $\\tilde{x}_1$ is 0 (just like the $n=1$ central moment).  The second standard moment is 1.  \n",
    "\n",
    " The skewness of a distribution can be positive, negative or zero.  A distribution that is symmetric about the mean will have zero skewness.\n",
    "\n",
    " The kurtosis measures the contributions from the tails of the distribution.  A Gaussian has a kurtosis of 3.\n",
    "\n",
    " The standard deviation, skewness and kurtosis are useful in that they enable characterization of a distribution without having a specific functional form.  In many real life cases this is very useful because there isn't a clear cut choice for the functional form, because it is useful to not have to resort to fitting to describe a distribution, or because there are various types of background in a distribution.\n",
    "\n",
    " Warning: there are a few different definitions of some of these observables running around.  Always check before blindly using a piece of code or a formula.  scipy.stats moments (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.moment.html) calculates **central moments** not the standard moments.  You should check that you get the answer you expect for a known test case too.\n",
    "\n",
    " For example, here is some code that calculates the first four central moments of a Gaussian and log-normal distribution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "4xVcwk17k4x9",
    "outputId": "db8765ab-445d-4373-f153-c8a8c0e14fae"
   },
   "outputs": [],
   "source": [
    "ntries = 1000\n",
    "gaussarray = np.array([])\n",
    "lognormarray = np.array([])\n",
    "for _ in range(ntries):\n",
    "\tvalue = np.random.lognormal(1, 1)\n",
    "\tlognormarray = np.append(lognormarray,value)\n",
    "\n",
    "for _ in range(ntries):\n",
    "\tvalue = gauss(0, 2)\n",
    "\tgaussarray = np.append(gaussarray,value)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Gauss and Lognormal')\n",
    "ax1.hist(gaussarray)\n",
    "ax2.hist(lognormarray, range=[0,10])\n",
    "\n",
    "print(\"Gaussian\")\n",
    "print(\"first central moment\", moment(gaussarray, moment=1))\n",
    "print(\"second central moment\", moment(gaussarray, moment=2))\n",
    "print(\"third central moment\", moment(gaussarray, moment=3))\n",
    "print(\"fourth central moment\", moment(gaussarray, moment=4))\n",
    "\n",
    "print(\"Log-normal\")\n",
    "print(\"first central moment\", moment(lognormarray, moment=1))\n",
    "print(\"second central moment\", moment(lognormarray, moment=2))\n",
    "print(\"third central moment\", moment(lognormarray, moment=3))\n",
    "print(\"fourth central moment\", moment(lognormarray, moment=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5CbP_tVp_9A"
   },
   "source": [
    "Now that we think we understand what the code is doing, we can look at the moments of the \"blobs\" dataset...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "4LbLt8NLMENx",
    "outputId": "ce120717-9f38-4d92-c146-95c8ceb233ec"
   },
   "outputs": [],
   "source": [
    "#always check the documentation to make sure the definitions are the same\n",
    "\n",
    "print(\"the first central moment ought to be 0:\", moment(blobs, moment=1))\n",
    "print(\"the second central moment ought to match with the variances we calculated with np.var()\", moment(blobs, moment=2))\n",
    "print(\"third central moment\", moment(blobs, moment=3))\n",
    "print(\"fourth central moment\", moment(blobs, moment=4))\n",
    "\n",
    "plt.hist(blobs['x2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fn5_obd4Tqf2"
   },
   "source": [
    "For the blobs dataset, the results might not be particularly illuminating, but the higher order moments can be particularly useful when you are trying to compare datasets for which you don't have a model (e.g. you don't know the dataset should be Gaussian or Poisson or ...) or you would just like to make a statement independent of models.  Here is an example from my past: https://arxiv.org/pdf/nucl-ex/0611019.pdf  In this case we measured a bunch of particle distributions and we wanted to compare their model independent way.\n",
    "\n",
    "More recently, here is a paper by our own Prof. Noronha-Hostler and collaborators proposing looking at the skewness of a distribution as a physics observable: https://inspirehep.net/literature/1791971.  This inspired the measurement of these moments by the ALICE Collaboration at the Large Hadron Collider here (https://arxiv.org/pdf/2308.16217.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38OzLvSXLXgh"
   },
   "source": [
    "**Mixed Moments**\n",
    "\n",
    "In addition to the moments of a single variable, for multivariate distributions, mixed-moments can be defined.  Taking the equation above:\n",
    "\n",
    "$$ \\Large\n",
    "\\langle g\\rangle \\equiv \\iint dx dy\\, g(x,y) P(x,y) \\; .\n",
    "$$\n",
    "\n",
    "if $g(x,y) = x^ny^m$ then $\\langle g\\rangle$ would be the raw moment of\n",
    "order $n$ of x and $m$ of y.  Central and standard mixed moments can be defined analogously to those for one variable above; for example the central moment of order $n$ of x and $m$ of y would be $g(x,y) = (x-\\bar{x})^n(y-\\bar{y})^m$ This can be generalized to an aritbrary number\n",
    "of variables.  \n",
    "\n",
    "This can seem esoteric, but for two variables and $m=n=1$, we\n",
    "will define the correlation and covariance below that is a key concept for\n",
    "interpreting data.  Additionally, my group has a recent paper about using\n",
    "moments to try to determine the fluctuations in jet measurements in heavy-ion\n",
    "collisions by measuring moments of the angular particle distributions (https://inspirehep.net/literature/2683577).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrXBaue7A7Kx"
   },
   "source": [
    "### <span style=\"color:LightGreen\">Covariance</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGpZZHOUDRiu"
   },
   "source": [
    "If you have more than one variable, a key question is whether having more than one variable providing new information or are the variables just repeating the same information?  Here will will introduce **correlation** and **covariance**.\n",
    "\n",
    "The **correlation between x and y** is defined as:\n",
    "$$ \\Large\n",
    "\\text{Corr}_{xy} \\equiv \\langle\\left( x - \\overline{x}\\right) \\left( y - \\overline{y}\\right)\\rangle \\; .\n",
    "$$\n",
    "\n",
    "A useful combination of the correlation and variances is the **correlation coefficient**,\n",
    "\n",
    "$$ \\Large\n",
    "\\rho_{xy} \\equiv \\frac{\\text{Corr}_{x,y}}{\\sigma_x \\sigma_y} \\; ,\n",
    "$$\n",
    "\n",
    "which, by construction, must be in the range $[-1, +1]$. Larger values of $|\\rho_{xy}|$ indicate that $x$ and $y$ are measuring related properties of the outcome so, together, carry less information than when $\\rho_{xy} \\simeq 0$. In the limit of $|\\rho_{xy}| = 1$, $y = y(x)$ is entirely determined by $x$, so carries no new information.\n",
    "\n",
    "We say that the variance and correlation are both *second-order moments* since they are expectation values of second-degree polynomials.\n",
    "\n",
    "We call the random variables $x$ and $y$ **uncorrelated** when:\n",
    "\n",
    "$$ \\Large\n",
    "x,y\\, \\text{uncorrelated}\\quad\\Rightarrow\\quad \\text{Corr}_{xy} = \\rho_{xy} = 0 \\; .\n",
    "$$\n",
    "\n",
    "To obtain an **empirical estimate** of the quantities above derived from your data, use the corresponding numpy functions, as we'll see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "V0FBPZFI09l9",
    "outputId": "b029ccd9-b5ee-49fd-d22c-edfb7a91c1d9"
   },
   "outputs": [],
   "source": [
    "plt.scatter(blobs['x0'], blobs['x1'], c=blobs['x2'], s=10)\n",
    "np.corrcoef(blobs['x0'], blobs['x1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXLbmCqRxjlB"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWsnVfHZU-kS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAJGDddVA7Kx"
   },
   "source": [
    "It is useful to construct the **covariance matrix** $C$ with elements:\n",
    "\n",
    "$$ \\Large\n",
    "C \\equiv \\begin{bmatrix}\n",
    "\\sigma_x^2 & \\rho_{x,y} \\sigma_x \\sigma_y \\\\\n",
    "\\rho_{x,y} \\sigma_x \\sigma_y & \\sigma_y^2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "With more than 2 random variables, $x_0, x_1, \\ldots$, this matrix generalizes to:\n",
    "\n",
    "$$ \\Large\n",
    "C_{ij} = \\langle \\left( x_i - \\overline{x}_i\\right) \\left( x_j - \\overline{x}_j\\right)\\rangle \\; .\n",
    "$$\n",
    "\n",
    "Comparing with the definitions above, we find variances on the diagonal:\n",
    "\n",
    "$$ \\Large\n",
    "C_{ii} = \\sigma_i^2\n",
    "$$\n",
    "\n",
    "and symmetric correlations on the off-diagonals:\n",
    "\n",
    "$$ \\Large\n",
    "C_{ij} = C_{ji} = \\rho_{ij} \\sigma_i \\sigma_j \\; .\n",
    "$$\n",
    "\n",
    "(The covariance is not only symmetric but also [positive definite](https://en.wikipedia.org/wiki/Positive-definite_matrix)).\n",
    "\n",
    "The covariance matrix is similar to a pairplot, with each $2\\times 2$ submatrix\n",
    "\n",
    "$$ \\Large\n",
    "\\begin{bmatrix}\n",
    "\\sigma_i^2 & \\rho_{ij} \\sigma_i \\sigma_j \\\\\n",
    "\\rho_{ij} \\sigma_i \\sigma_j & \\sigma_j^2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "describing a 2D elllipse in the $(x_i, x_j)$ plane:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZFuYXy6A7Kx"
   },
   "source": [
    "<img src=\"https://courses.physics.illinois.edu/phys398dap/fa2023/img/Statistics-Ellipse.png\" width=800 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHKW2BwnA7Kx"
   },
   "source": [
    "Note that you can directly read off the correlation coefficient $\\rho_{ij}$ from any of the points where the ellipse touches its bounding box. The ellipse rotation angle $\\theta$ is given by:\n",
    "\n",
    "$$ \\Large\n",
    "\\tan 2\\theta = \\frac{2 C_{ij}}{C_{ii} - C_{jj}} = \\frac{2\\rho_{ij}\\sigma_i\\sigma_j}{\\sigma_i^2 - \\sigma_j^2}\n",
    "$$\n",
    "\n",
    "and its [principal axes](https://en.wikipedia.org/wiki/Semi-major_and_semi-minor_axes) have lengths:\n",
    "\n",
    "$$ \\Large\n",
    "a_\\pm = 2\\sqrt{C_{ii} + C_{jj} \\pm d} \\quad \\text{with} \\quad\n",
    "d = \\sqrt{C_{11}^2 - 2 (1 - 2\\rho^2) C_{11} C_{22} + C_{22}^2} \\; .\n",
    "$$\n",
    "\n",
    "For practical calculations, the Singular Value Decomposition (SVD) of the covariance is useful:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EowzOpNBA7Kx"
   },
   "source": [
    "```\n",
    "U, s, _ = np.linalg.svd(C)\n",
    "theta = np.arctan2(U[1, 0], U[0, 0])\n",
    "ap, am = 2 * np.sqrt(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywqJaFZdA7Kx"
   },
   "source": [
    "The above assumes $C$ is $2\\times 2$. In the general $D\\times D$ case, use `C[[[j], [i]], [[j, i]]]` to pick out the $(i,j)$ $2\\times 2$ submatrix.\n",
    "\n",
    "Going back to the multivariate Gaussian of Week 2..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "aenHb0QC3Qk0",
    "outputId": "4c719d1a-6a79-4220-8fc3-a9f2a5208a55"
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import simpson\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "cov1 = 0.5\n",
    "mean, cov = [1, 2], [[1, cov1], [cov1, 1]] #means and covariance matrix\n",
    "z = np.random.multivariate_normal(mean, cov, 1000)\n",
    "print(z.shape)\n",
    "print(z[:5, :])\n",
    "\n",
    "fz, ex, ey = np.histogram2d(*z.T, bins=50, density=True)\n",
    "x = (ex[:-1] + ex[1:]) / 2\n",
    "y = (ey[:-1] + ey[1:]) / 2\n",
    "\n",
    "fx = simpson(fz, y, axis=1)\n",
    "fy = simpson(fz, x, axis=0)\n",
    "\n",
    "gs = GridSpec(2, 2, width_ratios=[3,1], height_ratios=[1,3])\n",
    "ax_fz = plt.subplot(gs[1,0])\n",
    "ax_fx = plt.subplot(gs[0,0], sharex=ax_fz)\n",
    "ax_fy = plt.subplot(gs[1,1], sharey=ax_fz)\n",
    "\n",
    "ax_fz.pcolormesh(x, y, fz.T)\n",
    "ax_fx.bar(x, fx)\n",
    "ax_fy.barh(y, fy)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "np.corrcoef(z[:,0],z[:,1]) #this gets the correlation coefficient\n",
    "np.cov(z[:,0],z[:,1]) # just a nice sanity check :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8RbK-73A7Kx",
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "**EXERCISE:** Calculate the empirical covariance matrix of the `blobs` dataset using `np.cov` (pay attention to the `rowvar` arg!). Next, calculate the ellipse rotation angles $\\theta$ in degrees for each of this matrix's 2D projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tL2gY3WWA7Kx",
    "outputId": "e5d44d70-f2d7-4cc7-8be9-cdd741668880",
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "C = np.cov(blobs, rowvar=False)\n",
    "print(C)\n",
    "for i in range(3):\n",
    "    for j in range(i + 1, 3):\n",
    "        U, s, _ = np.linalg.svd(C[[[j], [i]], [[j, i]]])\n",
    "        theta = np.arctan2(U[1, 0], U[0, 0])\n",
    "        print(i, j, np.round(np.degrees(theta), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fs2QWM4HA7Kx",
    "solution2": "shown"
   },
   "source": [
    "Recall that the `blobs` data was generated as a mixture of three Gaussian blobs, each with different parameters. However, describing a dataset with a single covariance matrix suggests a single Gaussian model. The lesson is that\n",
    "we can calculate an empirical covariance for *any* data, whether or not it is well described a single Gaussian."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
