{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Uncertainties and Fitting"
      ],
      "metadata": {
        "id": "6hCICzDV9pEs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhFwCOHXwqOS"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from random import seed\n",
        "from random import random\n",
        "from random import gauss\n",
        "import numpy as np\n",
        "from scipy.optimize import curve_fit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Counting Uncertainties**"
      ],
      "metadata": {
        "id": "jvHP9quq2HhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we're going to generate *ndatasets* of size *ntries* of random numbers distributed according to a Gaussian with $\\mu = 0$ and $\\sigma = 1$.  The distinction between *ndatasets* and *ntries* is artifical; we're really just generating *ndatasets x ntries* random numbers.  We're going to treat them as separate datasets though and see if we can understand how the datasets compare to each other.\n",
        "\n"
      ],
      "metadata": {
        "id": "5-GRORko-KOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#just generate the datasets\n",
        "ntries = 200\n",
        "ndatasets = 500\n",
        "data = np.random.normal(0,1,size=(ntries,ndatasets))\n",
        "#print(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "f8b6zOroxvqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by looking at a single dataset.  Maybe this is what Student A measured..."
      ],
      "metadata": {
        "id": "kucPWBuU2q7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nbins = 25\n",
        "counts, bins, bars  = plt.hist(data[:,0],bins=nbins,histtype='step')\n"
      ],
      "metadata": {
        "id": "xNe9KPPV2x1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many points are outside of $\\pm 1 \\sigma$?  How many are outside of $\\pm 2 \\sigma$.  Is that about what you expect based on the Gaussian distribution?\n",
        "\n",
        "About how many datasets would you expect to need to see a count outside of $\\pm 3 \\sigma$?"
      ],
      "metadata": {
        "id": "rnRSHLpV3BgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nbins = 25\n",
        "for i in range(3):\n",
        "  counts, bins, bars  = plt.hist(data[:,i],bins=nbins,histtype='step')\n"
      ],
      "metadata": {
        "id": "QTfzXUm63f8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've generated the datasets we want to just plot them.  If ndatasets is pretty small, we can just overlay them.  We see that they are all different (as we expect) but that they all roughly look like our Gaussian that we expect.\n",
        "\n",
        "The code below also puts the histogram bin content into a 2D array using *np.concatenate*.  This gives us an array which has *ntries* columns and *nbins* rows.\n",
        "\n",
        "If we look near the center of the Gaussian we expect to have a lot more counts and we should be able to see that the bin contents themselves are Gaussian distributed around their mean."
      ],
      "metadata": {
        "id": "M6VT7wn9_02C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nbins = 25\n",
        "counts, bins, bars  = plt.hist(data[:,0],bins=nbins,histtype='step')\n",
        "bincontent_array = counts.reshape(-1,1)\n",
        "guess=[1,1,1]\n",
        "uncertainties = np.sqrt(counts)\n",
        "bins = bins[:np.size(bins)-1]\n",
        "print(np.size(counts),np.size(bins))\n",
        "\n",
        "for i in range(ndatasets-1):\n",
        "  counts, bins, bars  = plt.hist(data[:,i+1],bins=nbins,histtype='step')\n",
        "  uncertainties = np.sqrt(counts)\n",
        "  bins = bins[:np.size(bins)-1]\n",
        "  bincontent_array = np.concatenate((counts.reshape(-1, 1), bincontent_array), axis=1)\n"
      ],
      "metadata": {
        "id": "bdasgkHv0zYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bincontent_array[12,:]\n",
        "plt.hist(bincontent_array[10,:],bins=20)\n",
        "plt.title(\"\")\n",
        "plt.legend([\"dataset\"])\n",
        "plt.show()\n",
        "print(bincontent_array[10,:].mean())\n",
        "print(bincontent_array[10,:].std())"
      ],
      "metadata": {
        "id": "MNenWBlC1v2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not too bad for what we can generate in class and we expect this estimation to get better as we increase both *ntries* and *ndatasets*.  "
      ],
      "metadata": {
        "id": "mhX4PqytCkI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Least Squares Fitting**"
      ],
      "metadata": {
        "id": "HAhAdJdw67c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Least squares fitting is implemented in scipy.  In order to use it, you define a function which is your functional form and fit parameters (here, *func*)."
      ],
      "metadata": {
        "id": "zcThrNk1CvvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func(x, a, b, c):\n",
        "    return a*np.exp(-0.5*(b-x)**2/(c**2))\n"
      ],
      "metadata": {
        "id": "ByG83InF5Yyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nbins = 20\n",
        "counts, bins, bars  = plt.hist(data[:,0],bins=nbins)\n",
        "guess=[100,0,1]\n",
        "uncertainties = np.sqrt(counts)\n",
        "bins = bins[:np.size(bins)-1]\n",
        "print(np.size(counts),np.size(bins))\n",
        "popt, pcov, infodict, mesg, ier = curve_fit(func, bins, counts,guess,uncertainties,full_output=True)\n",
        "fitparameters = popt.reshape(-1,1)\n",
        "for i in range(ndatasets-2):\n",
        "  counts, bins, bars  = plt.hist(data[:,i+1],bins=nbins)\n",
        "  uncertainties = np.sqrt(counts)\n",
        "  bins = bins[:np.size(bins)-1]\n",
        "  popt, pcov, infodict, mesg, ier = curve_fit(func, bins, counts,guess,uncertainties,full_output=True)\n",
        "  #print(ier, mesg)\n",
        "  if ier!=4: fitparameters = np.concatenate((popt.reshape(-1,1),fitparameters),axis=1)\n",
        "\n",
        "#print(fitparameters)\n"
      ],
      "metadata": {
        "id": "A4UI1DRP_aiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are going to use the fit for something, you had better check to make sure it converges!  Sometimes it does not!"
      ],
      "metadata": {
        "id": "PfL6XFZSHGsi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we're going to use: https://lmfit.github.io/lmfit-py/model.html scipy.optimize doesn't cut it here because there isn't a way to access the fit quality information without hand-coding the $\\chi^2$ calculation (happy to be shown to have just missed it)."
      ],
      "metadata": {
        "id": "wldAHxKwJx1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import exp, loadtxt, pi, sqrt\n",
        "!pip install lmfit\n",
        "from lmfit import Model\n",
        "\n",
        "\n",
        "x = bins\n",
        "y = counts\n",
        "\n",
        "\n",
        "def gaussian(x, amp, cen, wid):\n",
        "    \"\"\"1-d gaussian: gaussian(x, amp, cen, wid)\"\"\"\n",
        "    return (amp / (sqrt(2*pi) * wid)) * exp(-(x-cen)**2 / (2*wid**2))\n",
        "\n",
        "\n",
        "gmodel = Model(gaussian)\n",
        "result = gmodel.fit(y, x=x, weights=1.0/uncertainties, amp=5, cen=5, wid=1)\n",
        "\n",
        "print(result.fit_report())\n",
        "\n",
        "plt.plot(x, y, 'o')\n",
        "plt.plot(x, result.init_fit, '--', label='initial fit')\n",
        "plt.plot(x, result.best_fit, '-', label='best fit')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KrKuTF0a8EHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,3)\n",
        "fig.suptitle('fit results, amplitude, mean, and sigma')\n",
        "axs[0].hist(fitparameters[0,:],bins=20)\n",
        "axs[1].hist(fitparameters[1,:],bins=20)\n",
        "axs[2].hist(fitparameters[2,:],bins=20)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1AfYavUZ8vyA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}